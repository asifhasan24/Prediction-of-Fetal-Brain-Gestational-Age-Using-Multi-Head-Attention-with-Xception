{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOVG1fui6j+9ixZKOvhPDjn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asifhasan24/Prediction-of-Fetal-Brain-Gestational-Age-Using-Multi-Head-Attention-with-Xception/blob/main/Prediction_of_Fetal_Brain_Gestational_Age_Using_Multi_Head_Attention_with_Xception.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEL8WC1gaR3y"
      },
      "outputs": [],
      "source": [
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!pwd\n",
        "!pip install tensorflow-addons\n",
        "!kaggle datasets download -d asifhasan24/fetal-brain\n",
        "!unzip /content/fetal-brain.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications.xception import Xception, preprocess_input\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, MultiHeadAttention\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from tensorflow.keras.optimizers import RMSprop,Adam\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "image_folder_path = \"/content/images\"\n",
        "img_size = 75\n",
        "\n",
        "# Load the Excel file to get the mapping between folder names and labels\n",
        "excel_file_path = '/content/images/labels.xlsx'\n",
        "labels_df = pd.read_excel(excel_file_path)\n",
        "\n",
        "# Create a dictionary that maps each folder name to its label\n",
        "folder_to_label = dict(zip(labels_df['patient_id'], labels_df['ga_days']))\n",
        "\n",
        "# Use the new lists to create training, validation, and test sets\n",
        "train_img, val_img, test_img = [], [], []\n",
        "train_labels, val_labels, test_labels = [], [], []\n",
        "\n",
        "def enhance_image(image):\n",
        "    # Apply Unsharp Masking\n",
        "    gaussian_blur = cv2.GaussianBlur(image, (0, 0), 3)\n",
        "    unsharp_image = cv2.addWeighted(image, 1.5, gaussian_blur, -0.8, 0)\n",
        "    return unsharp_image\n",
        "\n",
        "for folder_name in folder_to_label.keys():\n",
        "    folder_path = os.path.join(image_folder_path, str(folder_name))\n",
        "    image_files = [file for file in os.listdir(folder_path) if file.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "    # Shuffle the list of image files for each patient\n",
        "    random.shuffle(image_files)\n",
        "\n",
        "    # Split the list of image files for each patient into training, validation, and testing sets\n",
        "    train_images, val_images, test_images = np.split(image_files, [int(0.7 * len(image_files)), int(0.8 * len(image_files))])\n",
        "\n",
        "    for image_file in train_images:\n",
        "        image_path = os.path.join(folder_path, image_file)\n",
        "        image = cv2.imread(image_path)\n",
        "        enhanced_image = enhance_image(image)\n",
        "        train_img.append(cv2.resize(enhanced_image, (img_size, img_size)))\n",
        "        train_labels.append(folder_to_label[folder_name])\n",
        "\n",
        "    for image_file in val_images:\n",
        "        image_path = os.path.join(folder_path, image_file)\n",
        "        image = cv2.imread(image_path)\n",
        "        enhanced_image = enhance_image(image)\n",
        "        val_img.append(cv2.resize(enhanced_image, (img_size, img_size)))\n",
        "        val_labels.append(folder_to_label[folder_name])\n",
        "\n",
        "    for image_file in test_images:\n",
        "        image_path = os.path.join(folder_path, image_file)\n",
        "        image = cv2.imread(image_path)\n",
        "        enhanced_image = enhance_image(image)\n",
        "        test_img.append(cv2.resize(enhanced_image, (img_size, img_size)))\n",
        "        test_labels.append(folder_to_label[folder_name])\n",
        "\n",
        "# Convert the lists to NumPy arrays\n",
        "train_img = np.array(train_img)\n",
        "train_labels = np.array(train_labels)\n",
        "val_img = np.array(val_img)\n",
        "val_labels = np.array(val_labels)\n",
        "test_img = np.array(test_img)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "train_img = preprocess_input(train_img)\n",
        "val_img = preprocess_input(val_img)\n",
        "test_img = preprocess_input(test_img)\n",
        "\n",
        "# Load the Xception model pre-trained on ImageNet data\n",
        "base_model = Xception(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
        "\n",
        "# Add a global spatial average pooling layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Reshape x to have sequence length dimension\n",
        "x = tf.expand_dims(x, axis=1)  # Shape: (None, 1, 2048)\n",
        "\n",
        "# Add multi-head attention layer\n",
        "num_heads = 8\n",
        "key_dim = 64\n",
        "value_dim = 64\n",
        "attention_output, attention_scores = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, value_dim=value_dim)(x, x, return_attention_scores=True)\n",
        "\n",
        "# Flatten the attention output tensor\n",
        "attention_output = tf.keras.layers.Flatten()(attention_output)\n",
        "\n",
        "# Add a fully-connected layer with 512 hidden units and relu activation\n",
        "x = Dense(512, activation='relu')(attention_output)\n",
        "\n",
        "# Add the output layer with one neuron for regression\n",
        "output_layer = Dense(1)(x)\n",
        "model = Model(inputs=base_model.input, outputs=output_layer)\n",
        "model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.001), metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_img, train_labels, epochs=100, batch_size=16, validation_data=(val_img, val_labels))\n",
        "\n",
        "# Evaluate on the test set\n",
        "y_pred = model.predict(test_img)\n",
        "r2 = r2_score(test_labels, y_pred)\n",
        "mae = mean_absolute_error(test_labels, y_pred)\n",
        "mse = mean_squared_error(test_labels, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(\"Test Set - R-squared score: {:.3f}\".format(r2))\n",
        "print(\"Test Set - MAE: {:.3f}\".format(mae))\n",
        "print(\"Test Set - MSE: {:.3f}\".format(mse))\n",
        "print(\"Test Set - RMSE: {:.3f}\".format(rmse))\n",
        "\n",
        "# Evaluate on the validation set\n",
        "val_pred = model.predict(val_img)\n",
        "val_r2 = r2_score(val_labels, val_pred)\n",
        "val_mae = mean_absolute_error(val_labels, val_pred)\n",
        "val_mse = mean_squared_error(val_labels, val_pred)\n",
        "val_rmse = np.sqrt(val_mse)\n",
        "print(\"Validation Set - R-squared score: {:.3f}\".format(val_r2))\n",
        "print(\"Validation Set - MAE: {:.3f}\".format(val_mae))\n",
        "print(\"Validation Set - MSE: {:.3f}\".format(val_mse))\n",
        "print(\"Validation Set - RMSE: {:.3f}\".format(val_rmse))\n"
      ],
      "metadata": {
        "id": "OYa7ZDiWfsoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZJD01zYo1w9U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}